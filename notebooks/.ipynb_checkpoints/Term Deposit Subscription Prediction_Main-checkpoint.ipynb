{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7824e4a-5c91-455a-a96c-d94a1fa4f7c2",
   "metadata": {},
   "source": [
    "# Term Deposit Subscription Prediction\n",
    "### Problem Statement:\n",
    "Predict whether a bank customer will subscribe to a term deposit as a result of a direct marketing campaign, based on various customer attributes and historical campaign data.\n",
    "\n",
    "### Objective:\n",
    "The primary objective is to build and evaluate classification models (e.g., Logistic Regression, Random Forest) to accurately predict term deposit subscriptions. This involves loading and exploring the dataset, preprocessing features, performing exploratory data analysis, training models, evaluating their performance using metrics like Confusion Matrix, F1-Score, ROC Curve, and Accuracy, and gaining insights into customer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707f69b-158a-449e-99f8-a039968c6b04",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb90e65-a734-42f4-9339-2f78a9810e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bed3c9-4f04-443d-b407-c30cfbefec33",
   "metadata": {},
   "source": [
    "## Load and clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ef797-e96d-470a-ade8-a5d014088d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/bank.csv\")\n",
    "print(\"Top 5 rows of dataset\\n\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Description\\n\")\n",
    "print(df.describe())\n",
    "# Remove leading/trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246f8ef-3bee-44fd-ae07-98638739c3b5",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- Features include demographics (age, job, education), financial data (balance, loan), and call info (contact, duration, poutcome).\n",
    "\n",
    "- Target column y is binary: 0 = no subscription, 1 = yes subscription.\n",
    "\n",
    "- Many categorical fields (e.g., job, contact, month) need encoding for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae97f17-ac9f-461c-a58f-77b9c07f7d32",
   "metadata": {},
   "source": [
    "## Clean and preprocess the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10ec66-69bc-4ffd-a913-4844719d7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Unique values before conversion:\", df['y'].unique())\n",
    "\n",
    "# Convert string '0'/'1' to integer 0/1\n",
    "df['y'] = df['y'].astype(int)\n",
    "\n",
    "print(\"Unique values after conversion:\", df['y'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6bbe2-2270-4dfa-9a4b-0211507404ef",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f3640-37ed-4d05-a715-db176aa023ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTarget class distribution:\")\n",
    "print(df['y'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize target distribution\n",
    "sns.countplot(x='y', data=df)\n",
    "plt.title(\"Target Class Distribution\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28dd98-6c81-4a74-baa3-13ff5e48c962",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- Class 0 (No subscription): 88.48%\n",
    "\n",
    "- Class 1 (Yes subscription): 11.52%\n",
    "\n",
    "**Data is highly imbalanced.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9472b9-3102-450b-adc4-a477f5c991b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric features\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574e37e-22fe-4bcd-adaa-9f8cdab4d48a",
   "metadata": {},
   "source": [
    "**Insights** \n",
    "- duration (0.40) is the strongest predictor of term deposit subscription — longer calls increase chances.\n",
    "\n",
    "- pdays (0.10) and previous (0.12) show weak positive correlation — past contact matters.\n",
    "\n",
    "- balance, age, campaign have low correlation with the target.\n",
    "\n",
    "- pdays and previous are moderately correlated (0.58) — possible multicollinearity.\n",
    "\n",
    "``Focus on duration, previous, and pdays for predictive power.``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd1b34a-bd18-4433-9241-7070c9d82153",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7585611-cfc4-4a59-9926-5a11bcd43077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encode categorical features\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df_encoded.drop('y', axis=1)\n",
    "y = df_encoded['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1aa61-de20-4b3d-af1b-63dba012d329",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd604f49-5e0a-4b43-994d-77877618ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1903c6-a0d7-46ae-a455-b9759d6136ef",
   "metadata": {},
   "source": [
    "## Feature Scaling with column names preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f915a-298b-4157-8e41-86882cd8be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e30cdd-88c0-409b-b287-a71168ac3e07",
   "metadata": {},
   "source": [
    "- It standardizes features by removing the mean and scaling to unit variance.\n",
    "- It ensures all features in X_train and X_test have the same scale, which helps models like Logistic Regression perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8287c1a",
   "metadata": {},
   "source": [
    "## Class Imbalance Handling: Class Weights and SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bace5c-c4aa-4b43-9082-c913a6ee6fae",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975b3f5",
   "metadata": {},
   "source": [
    "#### Logistic Regression with `class_weight='balanced'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_balanced = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "lr_balanced.fit(X_train, y_train)\n",
    "y_pred_lr_bal = lr_balanced.predict(X_test)\n",
    "print(\"Logistic Regression (Balanced)\")\n",
    "print(classification_report(y_test, y_pred_lr_bal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a235f-ad4e-4aae-99af-3f7c94aa38e1",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- Accuracy: 82%\n",
    "\n",
    "- High recall (0.78) for class 1 – detects positives well.\n",
    "\n",
    "- Low precision (0.37) – many false positives.\n",
    "\n",
    "- F1-score for class 1: 0.50 – decent balance for imbalanced data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549d296",
   "metadata": {},
   "source": [
    "#### Random Forest with `class_weight='balanced'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdca274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_balanced = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "rf_balanced.fit(X_train, y_train)\n",
    "y_pred_rf_bal = rf_balanced.predict(X_test)\n",
    "print(\"Random Forest (Balanced)\")\n",
    "print(classification_report(y_test, y_pred_rf_bal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ecd52b-7f20-4436-9502-bb658be6b668",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "- Accuracy: 89%\n",
    "\n",
    "- High precision (0.61) but very low recall (0.18) for class 1.\n",
    "\n",
    "- F1-score for class 1: 0.28 – poor at detecting positives despite high accuracy.\n",
    "\n",
    "- Biased toward majority class (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af0509",
   "metadata": {},
   "source": [
    "#### SMOTE Oversampling and Random Forest\n",
    "As SMOTE handle Tree Based Model Accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bb82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train Random Forest on SMOTE data\n",
    "rf_smote = RandomForestClassifier(random_state=42)\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_rf_smote = rf_smote.predict(X_test)\n",
    "\n",
    "print(\"Random Forest with SMOTE Oversampling\")\n",
    "print(classification_report(y_test, y_pred_rf_smote))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e193c-dcd3-4ef9-bf3f-8dcdaa9af0f0",
   "metadata": {},
   "source": [
    "**Inisghts:**\n",
    "- Accuracy: 88%\n",
    "\n",
    "- Improved recall (0.34) and precision (0.49) for class 1.\n",
    "\n",
    "- F1-score for class 1: 0.40 – better than RF alone.\n",
    "\n",
    "- More balanced performance, ideal for handling imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7306c-1e0d-495d-a62a-fc724b273e5f",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41625d25-b2c6-4b62-805f-b521fcff0e79",
   "metadata": {},
   "source": [
    "##### Logistic Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f609e4e-08da-47bb-b2dd-7ebf6da80e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions and probabilities\n",
    "log_preds = lr_balanced.predict(X_test)\n",
    "log_proba = lr_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"\\n--- Logistic Regression (Balanced) Evaluation ---\")\n",
    "print(\"F1 Score:\", f1_score(y_test, log_preds))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, log_proba))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, log_preds))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, log_preds), annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "plt.title(\"Logistic Regression (Balanced) - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test, log_proba)\n",
    "plt.plot(fpr_log, tpr_log, label=f\"Logistic Regression AUC = {roc_auc_score(y_test, log_proba):.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"ROC Curve - Logistic Regression (Balanced)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34bb26-14ac-4125-8464-63a502e42a3b",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "Overall prediction ability is decent **(AUC ≈ 0.89)**, but it struggles when it comes to detecting actual subscribers:\n",
    "\n",
    "- Class 1 (Subscribed) — precision 0.37, recall 0.78, F1-score 0.50\n",
    "\n",
    "- High number of false positives (137) compared to true subscribers correctly identified (81)\n",
    "\n",
    "- Class 0 (Not Subscribed) detection is strong: precision 0.93, recall 0.70, F1-score 0.80\n",
    "\n",
    "**Usage:**\n",
    "- Use this model when your priority is to not miss any interested customer.\n",
    "\n",
    "- Even if it means sending marketing offers to some people who won’t subscribe — that’s okay.\n",
    "\n",
    "- Best when you have resources to contact many people (like mass email or calls).\n",
    "\n",
    "\n",
    "**Logistic Regression**\n",
    "- **While excellent at predicting Class 0, it has poor precision for Class 1 (many false positives).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9ce50-e42b-4527-b505-4b466e5ebcec",
   "metadata": {},
   "source": [
    "##### Random Forest Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f5304-7540-4f02-9f79-aef04266c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and probabilities\n",
    "rf_preds = rf_balanced.predict(X_test)\n",
    "rf_proba = rf_balanced.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"\\n--- Random Forest (Balanced) Evaluation ---\")\n",
    "print(\"F1 Score:\", f1_score(y_test, rf_preds))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, rf_proba))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, rf_preds))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, rf_preds), annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "plt.title(\"Random Forest (Balanced) - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_proba)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest AUC = {roc_auc_score(y_test, rf_proba):.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"ROC Curve - Random Forest (Balanced)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d9f71-b371-428e-95ea-a2d2445d32a7",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "Best overall performance: highest AUC (≈ 0.90) and weighted F1-score (≈ 0.94)\n",
    "\n",
    "= Class 1 (Subscribed) — precision 0.61, recall 0.18, F1-score 0.28\n",
    "\n",
    "- Very few false positives (12), but many missed subscribers (false negatives: 85)\n",
    "\n",
    "- Class 0 (Not Subscribed) detection is excellent: precision 0.98, recall 0.99, F1-score 0.99\n",
    "\n",
    "**Usage:**\n",
    "- Use this model when you want to be very confident before reaching out.\n",
    "\n",
    "- It minimizes false offers, so you don’t waste marketing budget or annoy customers.\n",
    "\n",
    "- Best when you can only afford to target a few people and want them to be very likely to subscribe.\n",
    "\n",
    "**Random Forest (Balanced)**\n",
    "- **Excellent for Class 0 but very poor recall for Class 1 (misses most positive cases).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c504263-6ce7-4468-acda-d12924e58ef6",
   "metadata": {},
   "source": [
    "##### Random Forest (SMOTE) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f4a38-9ade-4378-a18d-dc90a3fd98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and probabilities\n",
    "rf_preds = rf_smote.predict(X_test)\n",
    "rf_proba = rf_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"\\n--- Random Forest (SMOTE) Evaluation ---\")\n",
    "print(\"F1 Score:\", f1_score(y_test, rf_preds))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, rf_proba))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, rf_preds))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, rf_preds), annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "plt.title(\"Random Forest (SMOTE) - Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_proba)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f\"Random Forest (SMOTE) AUC = {roc_auc_score(y_test, rf_proba):.2f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.title(\"ROC Curve - Random Forest (SMOTE)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080df058-816c-46fc-b871-80767e89b8e4",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "Better balance: boosts detection of actual subscribers with some trade-offs:\n",
    "\n",
    "- Class 1 (Subscribed) — precision 0.49, recall 0.34, F1-score 0.40\n",
    "\n",
    "- Recall improvement (18% → 34%) means it finds more real \"yes\" customers\n",
    "\n",
    "- More false positives than before (36 vs 12)\n",
    "\n",
    "- AUC slightly drops to ≈ 0.89, weighted F1 remains solid at ≈ 0.87\n",
    "\n",
    "**Usage:**\n",
    "- This model gives a better balance between reaching real subscribers and limiting mistakes.\n",
    "\n",
    "- Use it if you want to increase conversions, even if a few extra non-subscribers are contacted.\n",
    "\n",
    "- Good for mid-sized campaigns: you want reach + reasonable accuracy.\n",
    "\n",
    "**Random Forest (SMOTE)**\n",
    "- **Good performance for both classes with the most balanced results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a15b5-7883-403d-828e-b0839679cd3b",
   "metadata": {},
   "source": [
    "## Model Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f8fe3-329c-4632-b22a-70332e885d9e",
   "metadata": {},
   "source": [
    "After Testing all these model we made a conclusion that:\n",
    "**Random Forest (with SMOTE)** will best fit our problem requirement\n",
    "\n",
    "- It gives a good balance between recall (finding real subscribers) and precision (not guessing too many wrong ones).\n",
    "\n",
    "- It finds 35 actual subscribers (vs only 19 in plain Random Forest).\n",
    "\n",
    "- Yes, it makes some wrong guesses, but not too many (36 false positives).\n",
    "\n",
    "- F1-score and AUC are still strong (0.40 for \"yes\", and AUC ≈ 0.89).\n",
    "\n",
    "**Use Random Forest + SMOTE to:**\n",
    "\n",
    "- Target more potential subscribers.\n",
    "\n",
    "- Accept a small number of extra offers sent to uninterested customers — worth it if the term deposit is valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63c275-34c1-43ab-8079-39c0b8aa7673",
   "metadata": {},
   "source": [
    "## Model Explainability with SHAP\n",
    "To understand the influence of each feature on the predictions made by the best-performing model (Random Forest with SMOTE), we use SHAP (SHapley Additive exPlanations). This allows us to explain individual and global model behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ecb537-3ca1-4f5f-af84-7b09d730a894",
   "metadata": {},
   "source": [
    "## Random Forest + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c7c1b-dcec-40da-bd74-ed49b6c8c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SHAP TreeExplainer\n",
    "explainer = shap.TreeExplainer(rf_smote)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_probs = rf_smote.predict_proba(X_test)[:, 1]\n",
    "y_preds = rf_smote.predict(X_test)\n",
    "\n",
    "# Explain 5 individual predictions\n",
    "print(\"SHAP Explanation for 5 individual predictions:\")\n",
    "\n",
    "for i in range(5):\n",
    "    pred_class = y_preds[i]\n",
    "    actual_class = y_test.iloc[i]\n",
    "    pred_prob = y_probs[i]\n",
    "\n",
    "    print(f\"\\n🔹 Prediction {i+1} (Index {i})\")\n",
    "    print(f\"Predicted class: {pred_class} ({'subscription' if pred_class == 1 else 'no subscription'})\")\n",
    "    print(f\"Actual class: {actual_class}\")\n",
    "    print(f\"Predicted probability of subscription: {pred_prob:.2f}\")\n",
    "\n",
    "    display(shap.force_plot(\n",
    "        explainer.expected_value[1],\n",
    "        shap_values[1][i],\n",
    "        X_test.iloc[i],\n",
    "        matplotlib=True\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753de62-6c58-492b-86ce-8a0e0cb21020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SHAP TreeExplainer\n",
    "explainer = shap.TreeExplainer(rf_smote)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Get model predictions and probabilities\n",
    "y_probs = rf_smote.predict_proba(X_test)[:, 1]\n",
    "y_preds = rf_smote.predict(X_test)\n",
    "\n",
    "# Select indices where model predicts class 1 (subscription)\n",
    "class_1_indices = np.where(y_preds == 1)[0]\n",
    "\n",
    "print(\"SHAP Explanation for 5 instances predicted as class 1 (subscription):\")\n",
    "\n",
    "# Loop through first 5 predicted class 1 examples\n",
    "for i, idx in enumerate(class_1_indices[:5]):\n",
    "    pred_class = y_preds[idx]\n",
    "    actual_class = y_test.iloc[idx]\n",
    "    pred_prob = y_probs[idx]\n",
    "\n",
    "    print(f\"\\n🔹 Prediction {i+1} (Index {idx})\")\n",
    "    print(f\"Predicted class: {pred_class} (subscription)\")\n",
    "    print(f\"Actual class: {actual_class}\")\n",
    "    print(f\"Predicted probability of subscription: {pred_prob:.2f}\")\n",
    "\n",
    "    display(shap.force_plot(\n",
    "        explainer.expected_value[1],\n",
    "        shap_values[1][idx],\n",
    "        X_test.iloc[idx],\n",
    "        matplotlib=True\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b824b57-7f3b-4869-b2e8-3c181a2d5e7a",
   "metadata": {},
   "source": [
    "**Insights:**\n",
    "\n",
    "Based on these prediction Results:\n",
    "- **Subscription** is influenced by **long call duration, higher education, and positive past campaign outcomes**.\n",
    "\n",
    "- **Non-subscription** is driven by **short call duration, unknown contact methods, and contact during less effective months like May or July**.\n",
    "\n",
    "**Random Forest with SMOTE** effectively handles class imbalance and predicts non-subscriptions accurately.\n",
    "\n",
    "The model is slightly cautious, occasionally missing borderline \"Yes\" cases.\n",
    "\n",
    "Overall, it fits the task well by minimizing errors and offering interpretable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12f7b6-1f1d-4a79-8bdb-ed6bb99ea83e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- The objective was to predict whether a customer will subscribe to a term deposit using bank marketing data.\n",
    "\n",
    "- During analysis, we identified that the dataset was imbalanced, with significantly fewer \"Yes\" (subscription) outcomes compared to \"No\".\n",
    "\n",
    "- To address this, we applied techniques like class weight balancing and SMOTE (Synthetic Minority Oversampling Technique) to improve model fairness.\n",
    "\n",
    "- Among the tested models, **Random Forest with SMOTE** performed best, giving balanced results for both classes while maintaining good accuracy.\n",
    "\n",
    "- Although it made a few incorrect predictions, especially on borderline cases, it outperformed Logistic Regression and Random Forest with class weights.\n",
    "\n",
    "- Model explanations using SHAP revealed that:\n",
    "\n",
    "- Subscription is influenced by **long call duration, higher education, and favorable past outcomes**.\n",
    "\n",
    "- Non-subscription is associated with **short calls, unknown contact methods, and less effective months like May or July**.\n",
    "\n",
    "Overall, the **Random Forest + SMOTE** combination is effective and interpretable, making it well-suited for this prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cb57b-772f-4934-98c5-d37db829569b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
